Today was my first day starting the DP-203 Azure Data Engineering Course.

What I learned today :

	•	Cloud computing basically means renting someone else’s computer instead of managing your own servers.
	•	Azure has a ton of different services, but the main idea is that everything is grouped inside resource groups.
	•	A storage account is where Azure lets you store any kind of data — files, tables, logs, whatever.
	•	I also learned the difference between common data formats like CSV, JSON, Parquet, and Avro.
	•	Parquet is something I’ll definitely use a lot in data engineering because it’s fast and compressed.
	•	There are multiple storage types in Azure because different applications need different ways of reading/writing data.
	•	ADLS Gen2 is the main storage used in data engineering pipelines, especially for Databricks and Synapse.

Hands-on things I actually did today:

	•	Logged into Azure Portal and explored the layout
	•	Created a new resource group
	•	Created a storage account
	•	Turned on hierarchical namespace (this is what makes it ADLS Gen2)
	•	Created a container
	•	Uploaded a sample file just to see how everything works
	•	Accessed the storage explorer inside the portal and clicked around to understand how folders are structured.
  
My takeaways & understanding:
	•	Azure Portal isn’t as scary as it looks. Once you click around a bit, things start making sense.
	•	Data formats matter a LOT — Parquet and JSON will be everywhere in data engineering projects.
	•	ADLS Gen2 is going to be used heavily throughout the course, so I’m glad I got a feel for it today.
	•	Storage accounts feel like the foundation for almost everything related to data in Azure.