Day 16 - Azure Databricks & Azure Data Lake Gen2 security and Access

 I have been working on authorization and security for Azure Data Lake Gen2 and advanced Azure Databricks for ADLSGen2 security i understood how it uses azure active directory for authentication and learnt difference between authentication and authorization and i worked using azure storage explorer to connect ADLS Gen2 browse containers,upload files and check permissions instances i uploaded a test CSV files and verified i could access it and i understood SAS tokens and how they provide temporary limited access without sharing account keys and i learnt the three SAS types those are Account SAS,Service SAS and user delegation SAS and understood why they need careful handling for security and i revised microsoft EntraID basics and created a test user to see how new identities appear in role assignments then i got hands on with RBAC by assigning the storage Blob Data Reader role to a usee and testing it so that i could download files but not upload them which confirmed read only access.I also assigned the generic reader role and noticed it to view resources but doesn't grant any access to actual file data unless the storage and specific roles are added finally i learnt with Access control lists and learned they provide granular control at the directory and file level.i worked assigning READ,Write and Execute permissions for instance giving a user read and execute on a specific folder and let them list and read files but not modify them this really helped me understand that RBAC controls resource level access while ACLs control data level access.For Azure Databricks i worked on several advanced topics i created tables using Spark SQL and saved processed data back and delta tables i worked on loading external files directly using COPY INTO commands similar to Azure Synapse and i learnt streaming data from ADLS into Databricks which helped me to understand how spark structured streaming actually worksand i applied transformation filters,Selects and group by on live streaming data and learned wy defining schema early helps avoid performance issues i practoced sending databricks output to Azure synapse SQL using the connector which showed me how databricks fits into enterprise datapipelinea i learned about Delts lake powerful ffeatures ACID transactions,Version history,time travel and schema enforcemenr which are extreamly useful for analytics i also scheduled a databricks notebook as an automated job which helped me understand how production ETL pipelines run on schedules like daily or hourly.Finally i setup a spark stream to read real time events from event hub and process them inside databricks connecting streaming concepts across different azure services.