Day 13- Data processing with spark in Azure Synapse

I tried to load some sample datasets into spark dataframes and understood about lazy evaluation and spark doesn't process anything until it trigger an action like show() or count
() and i learnt dataframe transformations like select,filter and with column and learnt how spark splits data into partitions processed across executors in parallel.I worked with groupby operations and aggregrations like grouping the salesdata by region and calculating the totals and noticed how spark catalyst optimizer makes queries to run faster then i got into delta lake and learnt about ACID transcations,versioning and schema enforcement make it way more reliableand i saved a dataframe as a delts table in storage which is much better than regular files for production pipelines and i used spark to write processed data into synapse SQL pool.This shows how spark handles the large volume data processing while synapse manages analytical queries like everything is connecting from ingesting and transforming data with spark to storing it for analysis.